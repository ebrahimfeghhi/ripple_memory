{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09810d9e-d2a6-42af-ab73-a3dd2589743c",
   "metadata": {},
   "source": [
    "#### Theta-semantic distance correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d363f3bd-ddda-4d49-9835-ffe61e279788",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "model_full = KeyedVectors.load_word2vec_format('/home1/esolo/word2vec/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b48fe5b-a427-49c4-bfaf-0485c4ddf907",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.stats import zscore\n",
    "import sys\n",
    "sys.path.insert(0, '/home1/esolo/notebooks/codebases/')\n",
    "import CML_stim_pipeline\n",
    "from loc_toolbox import get_region, update_pairs\n",
    "from semantic_toolbox import remove_repeats, get_recall_clustering, find_temporal_runs\n",
    "import pickle as pk\n",
    "\n",
    "#CML data\n",
    "from cmlreaders import CMLReader, get_data_index\n",
    "df = get_data_index(\"r1\")\n",
    "\n",
    "#Get all subjects who did FR1\n",
    "FR_subs = df[df['experiment']=='FR1']['subject'].unique()\n",
    "\n",
    "def justfinites(arr):\n",
    "    return np.array(arr)[np.isfinite(arr)]\n",
    "\n",
    "def get_elecs_and_evs(reader, ref, ap_thresh=None):\n",
    "    evs = reader.load(\"events\")\n",
    "    contacts = reader.load(\"contacts\")\n",
    "    #pairs = reader.load(\"pairs\")\n",
    "    cmlpipe = CML_stim_pipeline.cml_pipeline(s, exp) #use this for loading electrodes with bad-elecs info\n",
    "    cmlpipe.set_elecs(type=ref)\n",
    "    pairs = cmlpipe.elecs\n",
    "    try:\n",
    "        locali = reader.load('localization')\n",
    "        pairs = update_pairs(locali, pairs)                \n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    mtl_elecs = get_region(pairs, region='hipp', side=None)\n",
    "    #mtl_elecs = pairs[pairs['ind.region'].str.contains('temporal').astype(bool) & (pairs['ind.x']!=0)]\n",
    "    if 'electrode_categories' in mtl_elecs.keys():  #some subjects dont have bad electrode info\n",
    "        mtl_elecs = mtl_elecs[~mtl_elecs['electrode_categories'].str.contains('bad')] #filter out bad electrodes\n",
    "        \n",
    "        #optionally, filter out epileptic electrodes\n",
    "        mtl_elecs = mtl_elecs[~mtl_elecs['electrode_categories'].str.contains('ictal')]\n",
    "        mtl_elecs = mtl_elecs[~mtl_elecs['electrode_categories'].str.contains('soz')]\n",
    "        \n",
    "    if ap_thresh is not None:\n",
    "        mtl_elecs = mtl_elecs[mtl_elecs['avg.y']>ap_thresh]\n",
    "\n",
    "    return mtl_elecs, evs\n",
    "\n",
    "def get_list_words(word_evs, listnum):\n",
    "    list_dat = word_evs[word_evs['list']==listnum]\n",
    "    words = np.array(list_dat['item_name'])\n",
    "    if 'AXE' in words:\n",
    "        words[words=='AXE']='AX'  #seems to not have this spelling of ax\n",
    "        \n",
    "    return words, list_dat\n",
    "\n",
    "def get_session_model(model, words, ndim):\n",
    "    \n",
    "    #Get PCA dims for *session-level* wordpool\n",
    "    words = np.array(words)\n",
    "    if 'AXE' in words:\n",
    "        words[words=='AXE']='AX'  #seems to not have this spelling of ax\n",
    "    word_mat = np.array([model[w.lower()] for w in words])\n",
    "    pca = PCA(n_components=ndim)\n",
    "    pcs = pca.fit_transform(word_mat)\n",
    "    exp_var = pca.explained_variance_ratio_\n",
    "    new_model = {}\n",
    "    for idx, w in enumerate(words):\n",
    "        new_model[w] = pcs[idx, :]\n",
    "        \n",
    "    return new_model, exp_var\n",
    "\n",
    "def get_session_PCs(word_evs, new_model):\n",
    "    \n",
    "    list_dat = word_evs[word_evs['list']==listnum]\n",
    "    list_words = np.array(list_dat['item_name'])\n",
    "    if 'AXE' in list_words:\n",
    "        list_words[list_words=='AXE']='AX'  #seems to not have this spelling of ax\n",
    "\n",
    "    #Get semantic positions from new_model\n",
    "    pcs = np.array([new_model[w_] for w_ in list_words])\n",
    "    \n",
    "    return pcs\n",
    "\n",
    "def regress_output_pos(all_theta, all_trans, all_outputs, all_serial_pos):\n",
    "    \n",
    "    #Regress out output position\n",
    "    import statsmodels.formula.api as sm\n",
    "\n",
    "    #Set up feature matrices\n",
    "    theta_feats = np.array(all_theta)[np.isfinite(all_trans)]\n",
    "    output_position = np.array(all_outputs)[np.isfinite(all_trans)]\n",
    "    all_serial_pos = np.array(all_serial_pos)[np.isfinite(all_trans)]\n",
    "    trans_feats = justfinites(all_trans)\n",
    "    X = np.empty([len(theta_feats), 4])\n",
    "    X[:, 0] = trans_feats\n",
    "    X[:, 1] = output_position\n",
    "    X[:, 2] = all_serial_pos\n",
    "    X[:, 3] = np.ones(len(theta_feats))\n",
    "    y = theta_feats\n",
    "\n",
    "    #Fit the model\n",
    "    result = sm.OLS(y, X).fit()\n",
    "    tru_coefs = copy(result.params)\n",
    "    \n",
    "    return tru_coefs\n",
    "\n",
    "def residualize_theta(all_theta, all_outputs, all_serial_pos):\n",
    "    \n",
    "    #Regress out output position\n",
    "    import statsmodels.formula.api as sm\n",
    "\n",
    "    #Set up feature matrices\n",
    "    theta_feats = np.array(all_theta)\n",
    "    output_position = np.array(all_outputs)\n",
    "    X = np.empty([len(theta_feats), 3])\n",
    "    X[:, 0] = output_position\n",
    "    X[:, 1] = np.array(all_serial_pos)\n",
    "    X[:, 2] = np.ones(len(theta_feats))\n",
    "    y = theta_feats\n",
    "\n",
    "    #Fit the model\n",
    "    result = sm.OLS(y, X).fit()\n",
    "    y_pred = result.predict(X)\n",
    "        \n",
    "    return all_theta-y_pred\n",
    "\n",
    "def residualize_con_by_power(con, power):\n",
    "    \n",
    "    import statsmodels.formula.api as sm\n",
    "    \n",
    "    #Set up feature matrices\n",
    "    con_feats = np.array(con)\n",
    "    pow_feats = np.array(power)\n",
    "    X = np.empty([len(con_feats), 2])\n",
    "    X[:, 0] = pow_feats\n",
    "    X[:, 1] = np.ones(len(con_feats))\n",
    "    y = con_feats\n",
    "    \n",
    "    #Fit the model\n",
    "    result = sm.OLS(y, X).fit()\n",
    "    y_pred = result.predict(X)\n",
    "    \n",
    "    return con_feats-y_pred\n",
    "    \n",
    "def get_power(reader, evs, mtl_elecs, freqs, ref):\n",
    "    \n",
    "    eeg = reader.load_eeg(events=evs, rel_start=-1000, rel_stop=0, scheme=mtl_elecs)  #Looking at 1 second prior to retrieval events\n",
    "\n",
    "    #Now get MNE format for power extraction\n",
    "    eeg = eeg.to_mne()\n",
    "    if ref=='mono':\n",
    "        eeg.set_eeg_reference(ref_channels=None) #Set to average reference\n",
    "        eeg.apply_proj()\n",
    "    eeg = eeg.resample(500)  #downsample to ___ Hz\n",
    "\n",
    "    #Use MNE to get multitaper power in theta and HFA bands\n",
    "    from mne.time_frequency import psd_multitaper\n",
    "    theta_pow, fdone = psd_multitaper(eeg, fmin=freqs[0], fmax=freqs[1], tmin=0.0, verbose=False)  #power is (events, elecs, freqs)\n",
    "    theta_pow = np.mean(np.log10(theta_pow), 1)  #Average across electrodes\n",
    "    \n",
    "    return theta_pow, fdone\n",
    "\n",
    "def semantic_temporal_model(sem_feats, temp_feats, theta_feats):\n",
    "\n",
    "    #Linear combination of semantic/temporal distances to predict theta power\n",
    "    import statsmodels.formula.api as sm\n",
    "    \n",
    "    #Set up feature matrices\n",
    "    X = np.empty([len(sem_feats), 3])\n",
    "    X[:, 0] = sem_feats\n",
    "    X[:, 1] = temp_feats\n",
    "    X[:, 2] = np.ones(len(sem_feats))\n",
    "    y = theta_feats\n",
    "    \n",
    "    #Fit the model\n",
    "    result = sm.OLS(y, X).fit()\n",
    "    \n",
    "    return result\n",
    "\n",
    "def get_long_short(arr, long_thresh=0.25, short_thresh=0.75):\n",
    "    \n",
    "    arr = np.array(arr)\n",
    "    longs = arr<long_thresh\n",
    "    num_longs = np.sum(longs)\n",
    "    \n",
    "    if num_longs<1:\n",
    "        raise ValueError('Not enough long transitions!')\n",
    "        \n",
    "    #srt_arr = np.argsort(arr)\n",
    "    #shorts = np.zeros(len(longs)).astype(bool)\n",
    "    #shorts[srt_arr[-1*num_longs:]] = True\n",
    "    \n",
    "    shorts = arr>short_thresh\n",
    "    \n",
    "    #print('Longs:'+str(np.sum(longs)))\n",
    "    #print('Shorts:'+str(np.sum(shorts)))\n",
    "    \n",
    "    return longs, shorts\n",
    "\n",
    "def get_hipp_phg_conn(reader, evs, mtl_elecs, freqs, ref):\n",
    "    \n",
    "    eeg = reader.load_eeg(events=evs, rel_start=-1000, rel_stop=0, scheme=mtl_elecs)  #Looking at 1 second prior to retrieval events\n",
    "\n",
    "    #Now get MNE format for power extraction\n",
    "    eeg = eeg.to_mne()\n",
    "    if ref=='mono':\n",
    "        eeg.set_eeg_reference(ref_channels=None) #Set to average reference\n",
    "        eeg.apply_proj()\n",
    "    sr = 500\n",
    "    eeg = eeg.resample(sr)  #downsample to ___ Hz\n",
    "    \n",
    "    #select only hippocampal elecs\n",
    "    hipp_filt = np.zeros(len(mtl_elecs))\n",
    "    for idx, i in enumerate(mtl_elecs['stein.region']):\n",
    "        if np.sum([(s_ in i) for s_ in ['CA', 'DG', 'SUB', 'Sub']])>0:\n",
    "            hipp_filt[idx] = 1\n",
    "    hipp_filt = hipp_filt.astype(bool)\n",
    "    if (np.sum(hipp_filt)==0) | np.sum(hipp_filt)==len(hipp_filt):  #don't run if no hipp/phg contacts\n",
    "        raise ValueError\n",
    "\n",
    "    #Use MNE to get multitaper power in theta and HFA bands\n",
    "    from mne.connectivity import spectral_connectivity\n",
    "    from scipy.special import logit\n",
    "    \n",
    "    #loop through events and get hipp/phg coherence\n",
    "    event_coh = []\n",
    "    for ev in range(len(eeg)):\n",
    "        con, fdone, times, n_epochs, n_tapers = spectral_connectivity(eeg[ev], method='coh', mode='multitaper', sfreq=sr, fmin=freqs[0], fmax=freqs[1], faverage=False, tmin=0.0, \n",
    "                                                                          mt_adaptive=False, verbose=False, mt_bandwidth=None)\n",
    "        #con = con[:, :, 0] #already averaged across frequencies\n",
    "        #con = np.sum([con, con.T], 0)\n",
    "        con_sym = np.empty(con.shape);\n",
    "        for fidx in range(con.shape[2]):\n",
    "            con_sym[:, :, fidx] = np.sum([con[:, :, fidx], con[:, :, fidx].T], 0)\n",
    "            tmp = con_sym[:, :, fidx]\n",
    "            tmp[np.diag_indices_from(tmp)] = np.nan\n",
    "            con_sym[:, :, fidx] = tmp\n",
    "        con = con_sym\n",
    "\n",
    "        cohs = []\n",
    "        for idxi, i in enumerate(hipp_filt):\n",
    "            if i:\n",
    "                for idxj, j in enumerate(~hipp_filt):\n",
    "                    if j:\n",
    "                        cohs.append(con[idxi, idxj, :])\n",
    "        cohs = np.array(cohs)\n",
    "\n",
    "        coh_mean = np.nanmean(logit(cohs), 0)\n",
    "        event_coh.append(coh_mean)\n",
    "    \n",
    "    return np.array(event_coh), fdone, hipp_filt\n",
    "\n",
    "def get_hipp_phg_plv(reader, evs, mtl_elecs, ref):\n",
    "    \n",
    "    #including a 1000 ms buffer because using wavelets\n",
    "    eeg = reader.load_eeg(events=evs, rel_start=-2000, rel_stop=1000, scheme=mtl_elecs)  #Looking at 1 second prior to retrieval events\n",
    "\n",
    "    #Now get MNE format for power extraction\n",
    "    eeg = eeg.to_mne()\n",
    "    if ref=='mono':\n",
    "        eeg.set_eeg_reference(ref_channels=None) #Set to average reference\n",
    "        eeg.apply_proj()\n",
    "    sr = 500\n",
    "    eeg = eeg.resample(sr)  #downsample to ___ Hz\n",
    "    \n",
    "    #select only hippocampal elecs\n",
    "    hipp_filt = np.zeros(len(mtl_elecs))\n",
    "    for idx, i in enumerate(mtl_elecs['stein.region']):\n",
    "        if np.sum([(s_ in i) for s_ in ['CA', 'DG', 'SUB', 'Sub']])>0:\n",
    "            hipp_filt[idx] = 1\n",
    "    hipp_filt = hipp_filt.astype(bool)\n",
    "    if (np.sum(hipp_filt)==0) | np.sum(hipp_filt)==len(hipp_filt):  #don't run if no hipp/phg contacts\n",
    "        raise ValueError\n",
    "\n",
    "    #Use MNE to get phase information\n",
    "    from mne.time_frequency import tfr_array_morlet\n",
    "    \n",
    "    #loop through events and get hipp/phg coherence\n",
    "    phases = tfr_array_morlet(eeg.get_data(), sfreq=sr, freqs=[4, 5, 6, 7, 8],\n",
    "                              n_cycles=5.0, output='phase', verbose=False)  #phase output is events x channels x freqs x time\n",
    "    phases = phases[:, :, :, int(sr*1.0):-1*int(sr*1.0)]\n",
    "    \n",
    "    #get plv over time\n",
    "    from pycircstat import resultant_vector_length, cdiff\n",
    "    from scipy.special import logit\n",
    "    \n",
    "    prs = []\n",
    "    for idxi, i in enumerate(hipp_filt):\n",
    "        if i:\n",
    "            for idxj, j in enumerate(~hipp_filt):\n",
    "                if j:\n",
    "                    diff = cdiff(phases[:, idxi, :, :], phases[:, idxj, :, :])\n",
    "                    prs.append(diff)\n",
    "    prs = np.array(prs)  #has dimensions pairs, events, freqs, time\n",
    "    plvs = resultant_vector_length(prs, axis=3)  #has dimensions pairs, events, freqs\n",
    "    plvs = np.mean(np.mean(plvs, 2), 0)  #has dimensions events \n",
    "\n",
    "    return plvs\n",
    "    \n",
    "\n",
    "def get_list_power(reader, ev, mtl_elecs, freqs, ref):\n",
    "    \n",
    "    eeg = reader.load_eeg(events=ev, rel_start=0, rel_stop=int(30*1000), scheme=mtl_elecs)  #Looking at 1 second prior to retrieval events\n",
    "\n",
    "    #Now get MNE format for power extraction\n",
    "    eeg = eeg.to_mne()\n",
    "    if ref=='mono':\n",
    "        eeg.set_eeg_reference(ref_channels=None) #Set to average reference\n",
    "        eeg.apply_proj()\n",
    "    eeg = eeg.resample(500)  #downsample to ___ Hz\n",
    "\n",
    "    #Use MNE to get multitaper power in theta and HFA bands\n",
    "    from mne.time_frequency import psd_multitaper\n",
    "    theta_pow, fdone = psd_multitaper(eeg, fmin=freqs[0], fmax=freqs[1], tmin=0.0, verbose=False)  #power is (events, elecs, freqs)\n",
    "    theta_pow = np.mean(np.log10(theta_pow), 1)  #Average across electrodes\n",
    "    \n",
    "    #Going to get a lot of needless frequencies, so just grab freqs in 1Hz intervals (every 30 units)\n",
    "    fdone = np.array(fdone)[::30]\n",
    "    theta_pow = theta_pow[0, ::30]\n",
    "    \n",
    "    return theta_pow, fdone\n",
    "\n",
    "def get_list_con(reader, evs, mtl_elecs, freqs, ref):\n",
    "    \n",
    "    eeg = reader.load_eeg(events=evs, rel_start=0, rel_stop=int(30*1000), scheme=mtl_elecs)  #Looking at 1 second prior to retrieval events\n",
    "\n",
    "    #Now get MNE format for power extraction\n",
    "    eeg = eeg.to_mne()\n",
    "    if ref=='mono':\n",
    "        eeg.set_eeg_reference(ref_channels=None) #Set to average reference\n",
    "        eeg.apply_proj()\n",
    "    sr = 500\n",
    "    eeg = eeg.resample(sr)  #downsample to ___ Hz\n",
    "    \n",
    "    #select only hippocampal elecs\n",
    "    hipp_filt = np.zeros(len(mtl_elecs))\n",
    "    for idx, i in enumerate(mtl_elecs['stein.region']):\n",
    "        if np.sum([(s_ in i) for s_ in ['CA', 'DG', 'SUB', 'Sub']])>0:\n",
    "            hipp_filt[idx] = 1\n",
    "    hipp_filt = hipp_filt.astype(bool)\n",
    "    if (np.sum(hipp_filt)==0) | np.sum(hipp_filt)==len(hipp_filt):  #don't run if no hipp/phg contacts\n",
    "        raise ValueError\n",
    "\n",
    "    #Use MNE to get multitaper power in theta and HFA bands\n",
    "    from mne.connectivity import spectral_connectivity\n",
    "    from scipy.special import logit\n",
    "    \n",
    "    #loop through events and get hipp/phg coherence\n",
    "    event_coh = []\n",
    "    for ev in range(len(eeg)):\n",
    "        con, fdone, times, n_epochs, n_tapers = spectral_connectivity(eeg[ev], method='coh', mode='multitaper', sfreq=sr, fmin=freqs[0], fmax=freqs[1], faverage=False, tmin=0.0, \n",
    "                                                                          mt_adaptive=False, verbose=False, mt_bandwidth=None)\n",
    "        #con = con[:, :, 0] #already averaged across frequencies\n",
    "        #con = np.sum([con, con.T], 0)\n",
    "        con_sym = np.empty(con.shape);\n",
    "        for fidx in range(con.shape[2]):\n",
    "            con_sym[:, :, fidx] = np.sum([con[:, :, fidx], con[:, :, fidx].T], 0)\n",
    "            tmp = con_sym[:, :, fidx]\n",
    "            tmp[np.diag_indices_from(tmp)] = np.nan\n",
    "            con_sym[:, :, fidx] = tmp\n",
    "        con = con_sym\n",
    "\n",
    "        cohs = []\n",
    "        for idxi, i in enumerate(hipp_filt):\n",
    "            if i:\n",
    "                for idxj, j in enumerate(~hipp_filt):\n",
    "                    if j:\n",
    "                        cohs.append(con[idxi, idxj, :])\n",
    "        cohs = np.array(cohs)\n",
    "\n",
    "        coh_mean = np.nanmean(logit(cohs), 0)\n",
    "        event_coh.append(coh_mean)\n",
    "    \n",
    "    return np.array(event_coh)[0, ::30], fdone[::30], hipp_filt\n",
    "    \n",
    "\n",
    "#load word2vec data\n",
    "model = pk.load(open('/home1/esolo/notebooks/Semantic_dimensions/wordpool_feats.pk', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525dd8d4-67d5-4ab5-9615-63e4da404d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pk\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.stats import zscore, pearsonr, spearmanr\n",
    "from scipy.spatial.distance import euclidean\n",
    "from scipy.stats.mstats import mquantiles\n",
    "from copy import copy\n",
    "import pandas as pd\n",
    "\n",
    "ndim = 11\n",
    "freq_range = [1, 150]\n",
    "ref_type = 'bi'\n",
    "do_conn = False\n",
    "\n",
    "res = pd.DataFrame()\n",
    "sub_database = pk.load(open('/scratch/esolo/Semantic_dimensions/subject_database.pkl', 'rb'))\n",
    "mtl_elecs = []\n",
    "\n",
    "for s in FR_subs[:]:\n",
    "    \n",
    "    #Load subjects information\n",
    "    exp = 'FR1'\n",
    "    sessions = df[np.logical_and(df[\"subject\"] == s, df['experiment']==exp)]['session'].unique()\n",
    "\n",
    "    for sess in sessions:\n",
    "        \n",
    "        sess_theta_r = np.empty((ndim-1, 150))  #should not hard-code frequency number, it's numfreqs*window(sec)\n",
    "        sess_theta_regress = np.empty(sess_theta_r.shape)\n",
    "        sess_theta_short = np.empty(sess_theta_r.shape)\n",
    "        sess_theta_long = np.empty(sess_theta_r.shape)\n",
    "        sess_con_r = np.empty(sess_theta_r.shape)\n",
    "        sess_con_long = np.empty(sess_theta_long.shape)\n",
    "        sess_con_short = np.empty(sess_theta_short.shape)\n",
    "    \n",
    "        loc = int(df[(df['subject']==s) & (df['session']==sess) & (df['experiment']==exp)]['localization'])\n",
    "        mont = int(df[(df['subject']==s) & (df['session']==sess) & (df['experiment']==exp)]['montage'])\n",
    "\n",
    "        #Get task events\n",
    "        try:\n",
    "            reader = CMLReader(s, exp, sess, montage=mont, localization=loc)\n",
    "\n",
    "            mtl_elecs, evs = get_elecs_and_evs(reader, ref_type, ap_thresh=None)\n",
    "            word_evs = evs[evs['type']=='WORD']\n",
    "            perf = np.sum(word_evs['recalled'])/float(len(word_evs))\n",
    "\n",
    "            if len(mtl_elecs)>0:\n",
    "                pass\n",
    "            else:\n",
    "                continue\n",
    "        except:\n",
    "            print('Unable to load MTL electrode/word info!')\n",
    "            continue\n",
    "\n",
    "        all_theta = []\n",
    "        all_trans = list([[] for i in range(ndim-1)])\n",
    "        all_outputs = []   \n",
    "        all_temp_trans = []\n",
    "        all_serial_pos = []\n",
    "        all_con = []\n",
    "\n",
    "        try:\n",
    "            new_model, exp_var = get_session_model(model_full, word_evs['item_name'], ndim)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        words_done = []  #for storing all the words seen so far\n",
    "        list_clustering = []\n",
    "        list_pows = []\n",
    "        list_con = []\n",
    "        list_perfs = []\n",
    "                \n",
    "        for listnum in word_evs['list'].unique()[:]:\n",
    "            try:\n",
    "\n",
    "                #Get info from one list\n",
    "                words, list_dat = get_list_words(word_evs, listnum)\n",
    "                words_done.extend(list(words))\n",
    "\n",
    "                #Get recall events and their semantic values \n",
    "                rec_evs = evs[(evs['type']=='REC_WORD') & (evs['list']==listnum) & (evs['intrusion']==0)]  #ignoring intrusions. should address repeat words.\n",
    "                serial_pos = [int(list_dat[list_dat['item_name']==w]['serialpos'])-1 for w in rec_evs['item_name']]\n",
    "                serial_pos, repeats_removed = remove_repeats(serial_pos)\n",
    "\n",
    "                tmp = np.ones(len(rec_evs)).astype(bool); tmp[repeats_removed] = False\n",
    "                if np.sum(tmp)<4: #threshold for list-level performance\n",
    "                    continue\n",
    "\n",
    "                #mask = find_temporal_runs(serial_pos)\n",
    "                mask = np.zeros(len(serial_pos)).astype(bool)\n",
    "                mask[0:] = True  #mask is constructed at the word level, not transition level\n",
    "\n",
    "                #Get percentile rank score for each transition \n",
    "                for dimidx, ndim_ in enumerate(range(1, ndim)):\n",
    "\n",
    "                    #Project semantic features for this list to N dimensions\n",
    "                    feats = np.array([model_full[w.lower()] for w in words])  #construct feature matrix from one list\n",
    "                    pca = PCA(n_components=ndim_)\n",
    "                    pcs = pca.fit_transform(feats)\n",
    "                    exp_var = pca.explained_variance_ratio_\n",
    "                    if ndim_==1:\n",
    "                        positions_1d = copy(pcs)\n",
    "\n",
    "                    #pcs = np.array([new_model[w] for w in words])[:, :ndim_]\n",
    "\n",
    "                    positions = pcs\n",
    "                    pcts = get_recall_clustering(positions, serial_pos) #Small number means short transition, spits out a \"closeness\" measure -- bigger numbers are closer items\n",
    "\n",
    "                    all_trans[dimidx].extend(np.array(pcts)[mask[:-1]])\n",
    "                    if ndim_==1:\n",
    "                        list_clustering.append(np.nanmean(pcts))\n",
    "\n",
    "                list_perfs.append(len(serial_pos)/12.)\n",
    "\n",
    "                #Also get temporal clustering\n",
    "                temp_coords = np.empty((len(positions), 1))\n",
    "                temp_coords[:, 0] = np.arange(len(positions))\n",
    "                temp_pcts = get_recall_clustering(temp_coords, serial_pos)\n",
    "                all_temp_trans.extend(np.array(temp_pcts)[mask[:-1]])\n",
    "\n",
    "                #Get pre-recall EEG power in region (1-second prior to recall)\n",
    "\n",
    "                #Get spectral power (and connectivity)\n",
    "                theta_pow, fdone = get_power(reader, rec_evs.iloc[tmp], mtl_elecs, freq_range, ref_type)\n",
    "                #list_pow, fdone_list = get_list_power(reader, evs[(evs['type']=='REC_START') & (evs['list']==listnum)].iloc()[0:1],\n",
    "                #                                     mtl_elecs, freq_range, ref_type)\n",
    "                list_pow = []\n",
    "\n",
    "                #con, fdone, _ = get_list_con(reader, evs[(evs['type']=='REC_START') & (evs['list']==listnum)].iloc()[0:1],\n",
    "                #                                     mtl_elecs, freq_range, ref_type)\n",
    "                list_con = []\n",
    "\n",
    "                list_pows.append(list_pow)\n",
    "                #list_con.append(con)\n",
    "\n",
    "                if do_conn:\n",
    "                    con = get_hipp_phg_plv(reader, rec_evs.iloc[tmp], mtl_elecs, ref_type)\n",
    "                else:\n",
    "                    con = []\n",
    "\n",
    "                #Aggregate data\n",
    "                all_theta.extend(list(theta_pow[mask, :][1:]))  #use 1: for within-transition theta, or :-1 for before-transition theta\n",
    "                all_outputs.extend(list(np.arange(0, len(theta_pow))[mask][:-1]))\n",
    "                all_serial_pos.extend(list(serial_pos[mask][:-1]))\n",
    "                if do_conn:\n",
    "                    all_con.extend(list(np.array(con)[mask][:-1]))\n",
    "                else:\n",
    "                    all_con.extend([])\n",
    "\n",
    "#                 print('all_theta: '+str(len(all_theta)))\n",
    "#                 print('all_outputs: '+str(len(all_outputs)))\n",
    "#                 print('all_serial_pos: '+str(len(all_serial_pos)))\n",
    "#                 print('all_temp_trans: '+str(len(all_temp_trans)))\n",
    "#                 print('all_trans: '+str(len(all_trans[0])))\n",
    "\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        if len(justfinites(all_trans[0]))<10:  #don't use sessions w/o at least N datapoints for the correlations\n",
    "            continue\n",
    "\n",
    "        #Correlate spectral power with transition distances\n",
    "        try:\n",
    "            all_theta = np.array(all_theta)\n",
    "            all_con = np.array(all_con)\n",
    "\n",
    "            #list-level metric\n",
    "            list_pows = np.array(list_pows)\n",
    "            list_clustering = np.array(list_clustering)\n",
    "            list_con = np.array(list_con)\n",
    "            list_perfs = np.array(list_perfs)\n",
    "\n",
    "            theta_avg_r = []; tru_avg_coefs = [];\n",
    "            con_avg_r = [];\n",
    "\n",
    "            for dimidx, dim_dat in enumerate(all_trans):  #all dimensions\n",
    "                for f in range(len(fdone)):  #all frequencies\n",
    "\n",
    "                    theta_r, theta_p = pearsonr(all_theta[np.isfinite(dim_dat), f], justfinites(dim_dat))\n",
    "                    sem_longs, sem_shorts = get_long_short(dim_dat, long_thresh=0.25, short_thresh=0.75)\n",
    "                    theta_short = np.nanmean( zscore(all_theta[:, f])[sem_shorts] )\n",
    "                    theta_long = np.nanmean( zscore(all_theta[:, f])[sem_longs] )\n",
    "                    if do_conn:\n",
    "                        if len(con.shape)==1:\n",
    "                            con_r, con_p = pearsonr(all_con[np.isfinite(dim_dat)], justfinites(dim_dat))\n",
    "                            con_short = np.nanmean( all_con[:][sem_shorts] )\n",
    "                            con_long = np.nanmean( all_con[:][sem_longs] ) \n",
    "                        else:\n",
    "                            con_r, con_p = pearsonr(all_con[np.isfinite(dim_dat), f], justfinites(dim_dat))\n",
    "                            con_short = np.nanmean( all_con[:, f][sem_shorts] )\n",
    "                            con_long = np.nanmean( all_con[:, f][sem_longs] )   #connectivity metrics already normalized\n",
    "                    else:\n",
    "                        con_r = np.nan\n",
    "                        con_short = np.nan\n",
    "                        con_long = np.nan\n",
    "\n",
    "                    tru_coefs = regress_output_pos(all_theta[np.isfinite(dim_dat), f], dim_dat, all_outputs, all_serial_pos)\n",
    "\n",
    "                    sess_theta_regress[dimidx, f] = tru_coefs[0]\n",
    "                    sess_theta_r[dimidx, f] = theta_r\n",
    "                    sess_theta_short[dimidx, f] = theta_short\n",
    "                    sess_theta_long[dimidx, f] = theta_long\n",
    "                    sess_con_r[dimidx, f] = con_r\n",
    "                    sess_con_short[dimidx, f] = con_short\n",
    "                    sess_con_long[dimidx, f] = con_long\n",
    "\n",
    "        except:\n",
    "            continue  #usually an EEG reading error, or not enough transitions\n",
    "        \n",
    "        try:\n",
    "            #Temporal clustering X frequency\n",
    "            temp_by_freq = []\n",
    "            temp_regress = []; temp_long = []; temp_short = []\n",
    "            con_temp = []; con_temp_long = []; con_temp_short = []\n",
    "            for f in range(len(fdone)):\n",
    "                temp_r, temp_p = pearsonr(all_theta[np.isfinite(all_temp_trans), f], justfinites(all_temp_trans))\n",
    "                temp_by_freq.append(temp_r)\n",
    "\n",
    "                if do_conn:\n",
    "                    if len(con.shape)==1:\n",
    "                        con_temp_r, _ = pearsonr(all_con[np.isfinite(all_temp_trans)], justfinites(all_temp_trans))\n",
    "                        con_temp.append(con_temp_r)\n",
    "                    else:\n",
    "                        con_temp_r, _ = pearsonr(all_con[np.isfinite(all_temp_trans), f], justfinites(all_temp_trans))\n",
    "                        con_temp.append(con_temp_r)\n",
    "                else:\n",
    "                    con_temp.append(np.nan)\n",
    "\n",
    "                coefs_temp = regress_output_pos(all_theta[np.isfinite(all_temp_trans), f], justfinites(all_temp_trans), all_outputs, all_serial_pos)\n",
    "                temp_regress.append(coefs_temp[0])\n",
    "\n",
    "                temp_longs, temp_shorts = get_long_short(all_temp_trans, long_thresh=0.25, short_thresh=0.75)\n",
    "                temp_long.append( np.nanmean( zscore(all_theta[:, f])[temp_longs] )  )\n",
    "                temp_short.append( np.nanmean( zscore(all_theta[:, f])[temp_shorts] )  )\n",
    "\n",
    "                if do_conn:\n",
    "                    if len(con.shape)==1:\n",
    "                        con_temp_long.append( np.nanmean( all_con[:][temp_longs] )   )\n",
    "                        con_temp_short.append(   np.nanmean( all_con[:][temp_shorts] ) )\n",
    "                    else:\n",
    "                        con_temp_long.append( np.nanmean( all_con[:, f][temp_longs] )   )\n",
    "                        con_temp_short.append(   np.nanmean( all_con[:, f][temp_shorts] ) )\n",
    "                else:\n",
    "                    con_temp_long.append(np.nan)\n",
    "                    con_temp_short.append(np.nan)\n",
    "\n",
    "\n",
    "            temp_by_freq = np.array(temp_by_freq)\n",
    "            con_temp = np.array(con_temp)\n",
    "            temp_regress = np.array(temp_regress)\n",
    "            temp_long = np.array(temp_long); temp_short = np.array(temp_short)\n",
    "            con_temp_long = np.array(con_temp_long)\n",
    "            con_temp_short = np.array(con_temp_short)\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "        sem_Z = sub_database[sub_database['subject']==s]['sem_Z']\n",
    "        tem_Z = sub_database[sub_database['subject']==s]['tem_Z']\n",
    "        res = res.append({'subject': s, 'localization': loc, 'montage': mont, 'session':sess,\n",
    "                        'r':sess_theta_r, 'r_regress':sess_theta_regress, 'power_short':sess_theta_short,\n",
    "                        'power_long':sess_theta_long, 'frequencies':fdone, 'dimensions':list(range(1, ndim)), \n",
    "                         'performance':perf, 'exp_var':exp_var, 'temporal_clustering_r': temp_by_freq,\n",
    "                         'semantic_transitions': all_trans[0], 'temporal_transitions': all_temp_trans,\n",
    "                         'temp_regress': temp_regress, 'temp_short': temp_short, 'temp_long': temp_long,\n",
    "                         'sem_Z':float(sem_Z), 'tem_Z':float(tem_Z), 'con_r': sess_con_r, 'con_sem_long': sess_con_long,\n",
    "                         'con_sem_short': sess_con_short, 'con_tem_long': con_temp_long, 'con_tem_short': con_temp_short, 'con_temp_r': con_temp,\n",
    "                         'list_pow':list_pows, 'list_clustering':list_clustering, 'list_con':list_con, 'list_perfs':list_perfs}, ignore_index=True)\n",
    "        \n",
    "\n",
    "    print(s)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
