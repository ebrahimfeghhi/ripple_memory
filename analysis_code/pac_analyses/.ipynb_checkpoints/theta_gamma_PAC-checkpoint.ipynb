{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/efeghhi/.conda/envs/env1/lib/python3.7/site-packages/ptsa/data/timeseries.py:526: FutureWarning: xarray subclass TimeSeriesX should explicitly define __slots__\n",
      "  class TimeSeriesX(TimeSeries):\n",
      "/home1/efeghhi/.conda/envs/env1/lib/python3.7/site-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "import sys \n",
    "sys.path.append('/home1/efeghhi/ripple_memory/analysis_code/')\n",
    "from load_data import *\n",
    "from analyze_data import *\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def permute_amplitude_time_series(amplitude_time_series):\n",
    "    \n",
    "     \n",
    "    '''\n",
    "    ndarray amplitude_time_series: array containing amplitude values over a given trial \n",
    "    \n",
    "    Circular permutes the input vector. \n",
    "    '''\n",
    "    \n",
    "    # Get the length of the time series\n",
    "    series_length = len(amplitude_time_series)\n",
    "    \n",
    "    min_cut_index = int(0.1 * series_length)\n",
    "    max_cut_index = int(0.9 * series_length )\n",
    "\n",
    "    # Choose a random index to cut the time series\n",
    "    cut_index = np.random.randint(min_cut_index, max_cut_index)\n",
    "\n",
    "    # Create the permuted time series by reversing the order of both parts\n",
    "    permuted_series = np.concatenate((amplitude_time_series[cut_index:], amplitude_time_series[:cut_index]))\n",
    "\n",
    "    return permuted_series\n",
    "\n",
    "def compute_gamma_theta_dist(pd):\n",
    "    \n",
    "    '''\n",
    "    dataframe pd: contains theta phase, low gamma, and high gamma values for a single trial\n",
    "    \n",
    "    This function groups high and low gamma values by theta phase to create a histogram. \n",
    "    It then normalizes this histogram such that the bins sum to 1.\n",
    "    '''\n",
    "    \n",
    "\n",
    "    low_gammas_by_theta = []\n",
    "    high_gammas_by_theta = []\n",
    "    \n",
    "    for theta_phase in np.unique(pd.theta):\n",
    "        \n",
    "        tp = pd.loc[pd.theta==theta_phase]\n",
    "        low_gammas_by_theta.append(np.mean(tp.lg))\n",
    "        high_gammas_by_theta.append(np.mean(tp.hg))\n",
    "        \n",
    "    lg_pdist = np.array(low_gammas_by_theta)/np.sum(low_gammas_by_theta)\n",
    "    hg_pdist = np.array(high_gammas_by_theta)/np.sum(high_gammas_by_theta)\n",
    "    \n",
    "    return lg_pdist, hg_pdist\n",
    "\n",
    "def compute_entropy(probability_arr):\n",
    "    \n",
    "    '''\n",
    "    ndarray probability_arr: 1 or 2d probability array\n",
    "    \n",
    "    If 1d array, returns entropy of the probability distribution.\n",
    "    If 2d array, returns entr\n",
    "    '''\n",
    "    \n",
    "    import math\n",
    "\n",
    "    if len(probability_arr.shape)==1:\n",
    "        entropy = -np.sum(probability_arr*np.log(probability_arr))\n",
    "    else:\n",
    "        # sum across bins (which are along the second axis) for permuted data\n",
    "        entropy = -np.sum(probability_arr*np.log(probability_arr), axis=1)\n",
    "    return entropy\n",
    "\n",
    "def compute_MI(dist, n_bins):\n",
    "    \n",
    "    '''\n",
    "    ndarray dist: probability distribution of gamma amplitudes grouped by theta phase.\n",
    "    For permutated data dist is of shape num_permutations x n_bins\n",
    "    int n_bins: number of theta phase bins used\n",
    "    \n",
    "    Computes modulation index, which is the entropy of the provided \n",
    "    '''\n",
    "    \n",
    "    \n",
    "    uniform_entropy = np.log(n_bins)\n",
    "    entropy_vals = compute_entropy(dist)\n",
    "    \n",
    "    MI = (uniform_entropy - entropy_vals)/uniform_entropy\n",
    "    return MI\n",
    "\n",
    "def compute_MOVI(distA, distB, n_bins):\n",
    "    \n",
    "    '''\n",
    "    ndarray distA, distB: probability distribution of gamma amplitudes grouped by theta phase.\n",
    "    For permutated data dist is of shape num_permutations x n_bins\n",
    "    int n_bins: number of theta phase bins used\n",
    "    \n",
    "    Computes MOVI, which is the MI of the difference of the two distributions\n",
    "    '''\n",
    "    \n",
    "    # from chanaz \n",
    "    dist_diff = ((distA - distB) + 2/n_bins)/2\n",
    "    MI_dist_diff = compute_MI(dist_diff, n_bins)\n",
    "    return MI_dist_diff\n",
    "\n",
    "def compute_p_value(MI, MI_permuted):\n",
    "    \n",
    "    '''\n",
    "    float MI: modulation index values from real data\n",
    "    ndarray MI_permuted: modulation index from permuted data\n",
    "    \n",
    "    Returns p-value based on permuted modulation index data  \n",
    "    '''\n",
    "    \n",
    "    p_value = 1 - np.argwhere(MI > MI_permuted).shape[0]/MI_permuted.shape[0]\n",
    "    return p_value\n",
    "\n",
    "\n",
    "# load data \n",
    "def load_pac_pd(encoding_mode):\n",
    "    \n",
    "    print(\"Loading data\")\n",
    "    \n",
    "    region_name = ['HPC']\n",
    "\n",
    "    condition_on_ca1_ripples = False\n",
    "    \n",
    "    if encoding_mode: \n",
    "        catFR_dir = '/scratch/efeghhi/catFR1/ENCODING/'\n",
    "    else:\n",
    "        catFR_dir = '/scratch/efeghhi/catFR1/IRIonly/'\n",
    "        \n",
    "    data_dict = load_data(catFR_dir, region_name=region_name, encoding_mode=encoding_mode, \n",
    "                        condition_on_ca1_ripples=condition_on_ca1_ripples)\n",
    "\n",
    "    if encoding_mode: \n",
    "        data_dict = remove_wrong_length_lists(data_dict)\n",
    "        \n",
    "    # ca1\n",
    "    ca1_elecs = [x for x in HPC_labels if 'ca1' in x]\n",
    "    data_dict_ca1 = select_region(data_dict, ca1_elecs)\n",
    "    count_num_trials(data_dict_ca1, \"ca1\")\n",
    "\n",
    "    data_dict_region = data_dict_ca1\n",
    "    \n",
    "    \n",
    "    sr = 500 # sampling rate in Hz\n",
    "    sr_factor = 1000/sr \n",
    "    \n",
    "    if encoding_mode:\n",
    "        # for encoding trials, neural recording \n",
    "        # starts from -700 ms before word onset \n",
    "        # and finishes 2300 ms post word onset \n",
    "        start_time = -700\n",
    "        end_time = 2300\n",
    "\n",
    "        # timepoints of interest\n",
    "        ripple_start = 400\n",
    "        ripple_end = 1100\n",
    "\n",
    "    else:\n",
    "        # for recall trials, neural recording \n",
    "        # starts from -200 ms before word onset \n",
    "        # and finishes 2000 ms post word onset \n",
    "        start_time = -2000\n",
    "        end_time = 2000\n",
    "        \n",
    "        # this is where we are interested in analyzing \n",
    "        # -600 to -100 ms (0 ms is word onset)\n",
    "        ripple_start = -600\n",
    "        ripple_end = -100\n",
    "\n",
    "    # convert to indices based on start time and sampling rate factor\n",
    "    ripple_start_idx = int((ripple_start - start_time)/sr_factor)\n",
    "    ripple_end_idx = int((ripple_end-start_time)/sr_factor)\n",
    "\n",
    "    data_dict_ca1.keys()\n",
    "\n",
    "    # create clustered int array\n",
    "    clustered_int = create_semantic_clustered_array(data_dict_region, encoding_mode)\n",
    "    data_dict_region['clust_int'] = clustered_int\n",
    "\n",
    "    dd_trials = dict_to_numpy(data_dict_region, order='C')\n",
    "\n",
    "    ripple_exists = create_ripple_exists(dd_trials, ripple_start_idx, ripple_end_idx, 0)\n",
    "    dd_trials['ripple_exists'] = ripple_exists\n",
    "    \n",
    "    # corresponds to 400 - 1100 ms post word onset if encoding\n",
    "    # or -600 to -100 ms before word recall if recalled \n",
    "    start_time = ripple_start_idx\n",
    "    end_time = ripple_end_idx\n",
    "    num_time_points = end_time - start_time\n",
    "    num_trials = len(dd_trials['theta_phase'])\n",
    "    n_bins = 18 # number of bins for theta phase \n",
    "\n",
    "    theta_raveled = np.ravel(dd_trials['theta_phase'][:, start_time:end_time])\n",
    "    theta_binned = np.floor((((theta_raveled + np.pi)/(2*np.pi))*n_bins))\n",
    "    high_gamma_raveled = np.ravel(dd_trials['high_gamma'][:, start_time:end_time])\n",
    "    low_gamma_raveled = np.ravel(dd_trials['low_gamma'][:, start_time:end_time])\n",
    "    trial_number = np.repeat(np.arange(num_trials), num_time_points)\n",
    "\n",
    "    clustered = np.repeat(dd_trials['clust_int'], num_time_points)\n",
    "    subj_ravel = np.repeat(dd_trials['subj'], num_time_points)\n",
    "    \n",
    "    if encoding_mode: \n",
    "        correct = np.repeat(dd_trials['correct'], num_time_points)\n",
    "    else:\n",
    "        # if using recalled data, just fill all trials with correct set to -1\n",
    "        # since there is no correct key\n",
    "        correct = -1*np.ones_like(clustered)\n",
    "        \n",
    "    regression_pd = pd.DataFrame({'theta':theta_binned, 'high_gamma': high_gamma_raveled, 'low_gamma':low_gamma_raveled, \n",
    "                        'correct': correct, 'clust': clustered, 'subj': subj_ravel, 'trial_num': trial_number})\n",
    "\n",
    "\n",
    "    return regression_pd, n_bins, num_time_points, np.unique(dd_trials['subj'])\n",
    "    \n",
    "            \n",
    "\n",
    "def compute_gamma_theta_dist(pd):\n",
    "    \n",
    "               \n",
    "    low_gammas_by_theta = []\n",
    "    high_gammas_by_theta = []\n",
    "    \n",
    "    for theta_phase in np.unique(pd.theta):\n",
    "        \n",
    "        tp = pd.loc[pd.theta==theta_phase]\n",
    "        low_gammas_by_theta.append(np.mean(tp.lg))\n",
    "        high_gammas_by_theta.append(np.mean(tp.hg))\n",
    "        \n",
    "    lg_pdist = np.array(low_gammas_by_theta)/np.sum(low_gammas_by_theta)\n",
    "    hg_pdist = np.array(high_gammas_by_theta)/np.sum(high_gammas_by_theta)\n",
    "    \n",
    "    return lg_pdist, hg_pdist\n",
    "        \n",
    "def compute_theta_gamma_PAC(single_subject, num_time_points, n_bins,\n",
    "                            num_permutations=300):\n",
    "    \n",
    "    num_trials_subj = np.unique(single_subject.trial_num).shape[0]\n",
    "\n",
    "    stored_dist_lg = np.zeros((num_trials_subj, n_bins))\n",
    "    stored_dist_permutations_lg = np.zeros((num_trials_subj, num_permutations, n_bins))\n",
    "    stored_dist_hg = np.zeros((num_trials_subj, n_bins))\n",
    "    stored_dist_permutations_hg = np.zeros((num_trials_subj, num_permutations, n_bins))\n",
    "\n",
    "    for idx, trial_num in enumerate(np.unique(single_subject.trial_num)):\n",
    "\n",
    "        print(f\"Trial {idx} out {np.unique(single_subject.trial_num).shape[0]}\")\n",
    "\n",
    "        trial_pd = single_subject.loc[single_subject.trial_num == trial_num]\n",
    "\n",
    "        original_pd = pd.DataFrame({'lg': trial_pd.low_gamma, 'hg': trial_pd.high_gamma, \n",
    "                                    'theta': trial_pd.theta})\n",
    "\n",
    "        # returns nbins shape array,\n",
    "        # where each entry is the gamma (low or high) \"probability\"\n",
    "        # at a given theta phase bin\n",
    "        lg_dist, hg_dist = compute_gamma_theta_dist(original_pd)\n",
    "\n",
    "        # store gamma distributions by theta phase for each trial \n",
    "        stored_dist_lg[idx] = lg_dist\n",
    "        stored_dist_hg[idx] = hg_dist\n",
    "\n",
    "        # for each trial, also compute a distribution when permuting the gamma \n",
    "        # arrays\n",
    "        for n in range(num_permutations):\n",
    "\n",
    "            low_gamma_permuted = permute_amplitude_time_series(trial_pd.low_gamma)\n",
    "            high_gamma_permuted = permute_amplitude_time_series(trial_pd.high_gamma)\n",
    "\n",
    "\n",
    "            permuted_pd = pd.DataFrame({'lg':low_gamma_permuted, 'hg':high_gamma_permuted, \n",
    "                                        'theta':trial_pd.theta})\n",
    "\n",
    "            lg_dist_p, hg_dist_p = compute_gamma_theta_dist(permuted_pd)\n",
    "\n",
    "            stored_dist_permutations_lg[idx, n] = lg_dist_p\n",
    "            stored_dist_permutations_hg[idx, n] = hg_dist_p\n",
    "\n",
    "    return stored_dist_lg, stored_dist_hg, stored_dist_permutations_lg, stored_dist_permutations_hg\n",
    "\n",
    "def finished_subjects(mode, metric, savePath = 'saved_results/'):\n",
    "    \n",
    "    '''\n",
    "    :param int mode: not correct (0), correct (1), unclustered (2), clustered (3) \n",
    "    :param str: MOVI or MI\n",
    "    :param str savePath: folder where previous results are stored to \n",
    "    '''\n",
    "    \n",
    "    # just load hg, because the subjects are the same\n",
    "    results= dict(np.load(f\"{savePath}hg_{metric}_by_subj_{mode}.npz\"))\n",
    "    \n",
    "    subjects_ran = sorted(results.keys())\n",
    "    \n",
    "    return subjects_ran \n",
    "    \n",
    "\n",
    "def save_MOVI(mode, savePath='/home1/efeghhi/ripple_memory/analysis_code/pac_analyses/'):\n",
    "    \n",
    "    low_gamma_MOVI_by_subj = {}\n",
    "    high_gamma_MOVI_by_subj = {}\n",
    "    low_gamma_MOVI_z_by_subj = {}\n",
    "    high_gamma_MOVI_z_by_subj = {}\n",
    "    \n",
    "    low_gamma_p_vals_by_subj = {}\n",
    "    high_gamma_p_vals_by_subj = {}\n",
    "    \n",
    "    pac_pd_encoding, n_bins, num_time_points_encoding, subj_encoding = load_pac_pd(1)\n",
    "    pac_pd_recalled, _, num_time_points_recalled, subj_recalled = load_pac_pd(0)\n",
    "    \n",
    "    if mode == 2:\n",
    "        pac_pd_encoding = pac_pd_encoding.loc[pac_pd_encoding.clust==0]\n",
    "        pac_pd_recalled = pac_pd_recalled.loc[pac_pd_recalled.clust==0]\n",
    "    if mode == 3:\n",
    "        pac_pd_encoding = pac_pd_encoding.loc[pac_pd_encoding.clust==1]\n",
    "        pac_pd_recalled = pac_pd_recalled.loc[pac_pd_recalled.clust==1]\n",
    "        \n",
    "    subj_both = np.intersect1d(subj_encoding, subj_recalled) \n",
    "    \n",
    "    min_num_trials = 30\n",
    "    \n",
    "    subjects_ran = finished_subjects(mode, 'MOVI') \n",
    "        \n",
    "    for subj in subj_both:\n",
    "        \n",
    "        print(f\"Subject: {subj}\")\n",
    "        \n",
    "        if subj in subjects_ran:\n",
    "            print(\"Ran subject already, skipping\")\n",
    "            continue\n",
    "\n",
    "        single_subject_encoding = pac_pd_encoding.loc[pac_pd_encoding.subj==subj]\n",
    "        single_subject_recalled = pac_pd_recalled.loc[pac_pd_recalled.subj==subj]\n",
    "\n",
    "        num_trials_subj_encoding = int(single_subject_encoding.high_gamma.shape[0]/num_time_points_encoding)\n",
    "        num_trials_subj_recalled = int(single_subject_recalled.high_gamma.shape[0]/num_time_points_recalled)\n",
    "        \n",
    "        if min(num_trials_subj_encoding, num_trials_subj_recalled) < int(min_num_trials):\n",
    "\n",
    "            print(\"trial count too low, skipping subject\")\n",
    "            continue\n",
    "            \n",
    "        print(\"ENCODING\")\n",
    "        theta_gamma_hist_encoding = compute_theta_gamma_PAC(single_subject_encoding,\n",
    "                                                            num_time_points_encoding, n_bins)\n",
    "        print(\"RECALLED\")\n",
    "        theta_gamma_hist_recalled = compute_theta_gamma_PAC(single_subject_recalled, \n",
    "                                                            num_time_points_recalled, n_bins)\n",
    "                                                 \n",
    "        \n",
    "        stored_dist_lg_encoding = theta_gamma_hist_encoding[0]\n",
    "        stored_dist_hg_encoding = theta_gamma_hist_encoding[1]\n",
    "        stored_dist_permutations_lg_encoding = theta_gamma_hist_encoding[2]\n",
    "        stored_dist_permutations_hg_encoding = theta_gamma_hist_encoding[3]\n",
    "        \n",
    "        stored_dist_lg_recalled = theta_gamma_hist_recalled[0]\n",
    "        stored_dist_hg_recalled = theta_gamma_hist_recalled[1]\n",
    "        stored_dist_permutations_lg_recalled = theta_gamma_hist_recalled[2]\n",
    "        stored_dist_permutations_hg_recalled = theta_gamma_hist_recalled[3]\n",
    "        \n",
    "        MOVI_lg = compute_MOVI(np.mean(stored_dist_lg_encoding, axis=0), \n",
    "                               np.mean(stored_dist_lg_recalled, axis=0), n_bins)\n",
    "        MOVI_hg = compute_MOVI(np.mean(stored_dist_hg_encoding, axis=0), \n",
    "                               np.mean(stored_dist_hg_recalled, axis=0), n_bins)\n",
    "        \n",
    "        MOVI_lg_p = compute_MOVI(np.mean(stored_dist_permutations_hg_encoding, axis=0), \n",
    "                               np.mean(stored_dist_permutations_lg_recalled, axis=0), n_bins)\n",
    "        MOVI_hg_p = compute_MOVI(np.mean(stored_dist_permutations_hg_encoding, axis=0), \n",
    "                               np.mean(stored_dist_permutations_hg_recalled, axis=0), n_bins)\n",
    "        \n",
    "        # compute p-value as fraction of permuted MI values that are higher\n",
    "        # than or equal to the MI value obtained with the real data \n",
    "        p_val_lg = compute_p_value(MOVI_lg, MOVI_lg_p)\n",
    "        p_val_hg = compute_p_value(MOVI_hg, MOVI_hg_p)\n",
    "        \n",
    "        low_gamma_p_vals_by_subj[subj] = p_val_lg\n",
    "\n",
    "        high_gamma_p_vals_by_subj[subj] = p_val_hg\n",
    "\n",
    "        print(\"P val: \", p_val_lg, p_val_hg)\n",
    "        print(\"MOVI: \", MOVI_lg, MOVI_hg)\n",
    "\n",
    "        # save MI for non-permuted data\n",
    "        low_gamma_MOVI_by_subj[subj] = MOVI_lg\n",
    "        high_gamma_MOVI_by_subj[subj] = MOVI_hg\n",
    "\n",
    "        # save MI normalized by null distribution\n",
    "        low_gamma_MOVI_z_by_subj[subj] = (MOVI_lg - np.mean(MOVI_lg_p))/np.std(MOVI_lg_p)\n",
    "        high_gamma_MOVI_z_by_subj[subj] = (MOVI_hg - np.mean(MOVI_hg_p))/np.std(MOVI_hg_p)\n",
    "\n",
    "\n",
    "        np.savez(f'{savePath}lg_MOVI_by_subj_{mode}', **low_gamma_MOVI_by_subj)\n",
    "        np.savez(f'{savePath}hg_MOVI_by_subj_{mode}', **high_gamma_MOVI_by_subj)\n",
    "\n",
    "        np.savez(f'{savePath}lg_MOVI_by_subj_z_{mode}', **low_gamma_MOVI_z_by_subj)\n",
    "        np.savez(f'{savePath}hg_MOVI_by_subj_z_{mode}', **high_gamma_MOVI_z_by_subj)\n",
    "\n",
    "        np.savez(f'{savePath}lg_MOVI_p_vals_by_subj_{mode}', **low_gamma_p_vals_by_subj)\n",
    "        np.savez(f'{savePath}hg_MOVI_p_vals_by_subj_{mode}', **high_gamma_p_vals_by_subj)\n",
    "\n",
    "        \n",
    "def save_MI(encoding_mode, mode, savePath='/home1/efeghhi/ripple_memory/analysis_code/pac_analyses/'):\n",
    "    \n",
    "    '''\n",
    "    :param int encoding_mode: 1 for encoding, 0 for recall data \n",
    "    :param int mode: 0 for correct trials, 1 for incorrect, \n",
    "    2 for not clustered, 3 for clustered. Note for recall data only modes\n",
    "    2 and 3 can be passed.\n",
    "    '''\n",
    "\n",
    "    pac_pd, n_bins, num_time_points, subj_arr = load_pac_pd(encoding_mode)\n",
    "    \n",
    "    min_num_trials = 30\n",
    "    \n",
    "    if mode == 0 or mode == 1:\n",
    "        if encoding_mode == 0:\n",
    "            print(\"Cannot pass modes 0 and 1 for recalled data\")\n",
    "            return 0 \n",
    "    \n",
    "    if mode == 0:\n",
    "        pac_pd = pac_pd.loc[pac_pd.correct==0]\n",
    "    if mode == 1:\n",
    "        pac_pd = pac_pd.loc[pac_pd.correct==1]\n",
    "    if mode == 2:\n",
    "        pac_pd = pac_pd.loc[pac_pd.clust==0]\n",
    "    if mode == 3:\n",
    "        pac_pd = pac_pd.loc[pac_pd.clust==1]\n",
    "    \n",
    "    # MI stands for modulation index\n",
    "    # which is (log(n_bins) - H(p))/log(n_bins), which is 0 if the PAC\n",
    "    # histogram has the same entropy as the uniform distribution, and b/w 0 and 1\n",
    "    # to the extent its entropy is less than the uniform distribution\n",
    "    \n",
    "    low_gamma_MI_by_subj = {}\n",
    "    high_gamma_MI_by_subj = {}\n",
    "    low_gamma_MI_z_by_subj = {}\n",
    "    high_gamma_MI_z_by_subj = {}\n",
    "    \n",
    "    low_gamma_p_vals_by_subj = {}\n",
    "    high_gamma_p_vals_by_subj = {}\n",
    "    \n",
    "    for subj in subj_arr:\n",
    "    \n",
    "        \n",
    "        print(f\"Subject: {subj}\")\n",
    "\n",
    "        single_subject = pac_pd.loc[pac_pd.subj==subj]\n",
    "\n",
    "        num_trials_subj = int(single_subject.high_gamma.shape[0]/num_time_points)\n",
    "\n",
    "        if num_trials_subj < int(min_num_trials):\n",
    "\n",
    "            print(\"trial count too low, skipping subject\")\n",
    "            continue\n",
    "            \n",
    "        theta_gamma_hist = compute_theta_gamma_PAC(single_subject, \n",
    "                                                  num_time_points, n_bins)\n",
    "        \n",
    "        stored_dist_lg = theta_gamma_hist[0]\n",
    "        stored_dist_hg = theta_gamma_hist[1]\n",
    "        stored_dist_permutations_lg = theta_gamma_hist[2]\n",
    "        stored_dist_permutations_hg = theta_gamma_hist[3]\n",
    "\n",
    "        # average aross distributions generated across trials for a given participant \n",
    "        # and compute MI\n",
    "        MI_lg = compute_MI(np.mean(stored_dist_lg, axis=0), n_bins)\n",
    "        MI_hg = compute_MI(np.mean(stored_dist_hg, axis=0), n_bins)\n",
    "\n",
    "        # also do the same for permuted data. For permuted data, when we average across the trials \n",
    "        # the phase amplitude coupling should cancel out because each trial should be coupled to \n",
    "        # a distinct theta phase value because of the coupling. The resulting output will be of shape\n",
    "        # num_permutations. \n",
    "        MI_lg_p = compute_MI(np.mean(stored_dist_permutations_lg, axis=0), n_bins)\n",
    "        MI_hg_p = compute_MI(np.mean(stored_dist_permutations_hg, axis=0), n_bins)\n",
    "\n",
    "        # compute p-value as fraction of permuted MI values that are higher\n",
    "        # than or equal to the MI value obtained with the real data \n",
    "        p_val_lg = compute_p_value(MI_lg, MI_lg_p)\n",
    "        p_val_hg = compute_p_value(MI_hg, MI_hg_p)\n",
    "\n",
    "        low_gamma_p_vals_by_subj[subj] = p_val_lg\n",
    "\n",
    "        high_gamma_p_vals_by_subj[subj] = p_val_hg\n",
    "\n",
    "        print(\"P val: \", p_val_lg, p_val_hg)\n",
    "        print(\"MI: \", MI_lg, MI_hg)\n",
    "\n",
    "        # save MI for non-permuted data\n",
    "        low_gamma_MI_by_subj[subj] = MI_lg\n",
    "        high_gamma_MI_by_subj[subj] = MI_hg\n",
    "\n",
    "        # save MI normalized by null distribution\n",
    "        low_gamma_MI_z_by_subj[subj] = (MI_lg - np.mean(MI_lg_p))/np.std(MI_lg_p)\n",
    "        high_gamma_MI_z_by_subj[subj] = (MI_hg - np.mean(MI_hg_p))/np.std(MI_hg_p)\n",
    "\n",
    "        if encoding_mode == 0:\n",
    "            recall_str = \"recalled\"\n",
    "        else:\n",
    "            recall_str = \"\"\n",
    "\n",
    "        np.savez(f'{savePath}lg_MI_by_subj_{mode}{recall_str}', **low_gamma_MI_by_subj)\n",
    "        np.savez(f'{savePath}hg_MI_by_subj_{mode}{recall_str}', **high_gamma_MI_by_subj)\n",
    "\n",
    "        np.savez(f'{savePath}lg_MI_by_subj_z_{mode}{recall_str}', **low_gamma_MI_z_by_subj)\n",
    "        np.savez(f'{savePath}hg_MI_by_subj_z_{mode}{recall_str}', **high_gamma_MI_z_by_subj)\n",
    "\n",
    "        np.savez(f'{savePath}lg_MI_p_vals_by_subj_{mode}{recall_str}', **low_gamma_p_vals_by_subj)\n",
    "        np.savez(f'{savePath}hg_MI_p_vals_by_subj_{mode}{recall_str}', **high_gamma_p_vals_by_subj)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['R1045E', 'R1061T', 'R1065J', 'R1108J', 'R1131M', 'R1144E', 'R1157C', 'R1204T', 'R1207J', 'R1236J', 'R1278E', 'R1310J', 'R1315T', 'R1330D', 'R1332M', 'R1338T', 'R1354E', 'R1367D', 'R1380D', 'R1383J', 'R1385E', 'R1397D', 'R1422T']\n"
     ]
    }
   ],
   "source": [
    "finished_subjects(2, 'MOVI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python env1 eb",
   "language": "python",
   "name": "env1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
